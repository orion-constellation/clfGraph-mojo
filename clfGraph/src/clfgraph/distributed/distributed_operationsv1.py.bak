'''
DISTRIBUTED OPERATIONS WITH PY 
- Operations for distributed computing on Ray (GCP or Ray)


'''
import ray
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric import GCNConv

# Ray init
ray.init(ignore_reinit_error=True)


class GNNExpert(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GNNExpert, self).__init__()
        self.conv1 = GCNConv(in_channels, 128)
        self.conv2 = GCNConv(128, out_channels)
        self.conv2 = GCNConv(128, out_channels)
            
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)
    
@ray.remote
class DistributedExpert:
    def __init__(self, in_channels, out_channels):
        self.model = GNNExpert(in_channels, out_channels)
    
    def train(self, data, optimizer):
        self.model.train()
        optimizer.zero_grad()
        output = self.model(data)
        loss = F.nll_loss(output[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()
        return loss.item()
    
    def inference(self, data):
        self.model.eval()
        with torch.no.grad():
            output = self.model(data)
        return output.argmax(dim=1)
    
    def distributed_train(experts, data_list, lr=0.01):
        optimizers = [torch.optim.Adam(experts.model.parameters(), lr=lr) for experts in experts]
        futures = [expert.train.remote(data, optimizer) for expert, data, optimizer in zip(experts, data_list, optimizers)]
        results = ray.get(futures)
        print(f"Results: {results}")
    
    def distributed_inference(experts, data_list):
        futures = [expert.inference.remote(data) for expert, data in zip(experts, data_list)]
        results = ray.get(futures)
        return results
   
                
            
    