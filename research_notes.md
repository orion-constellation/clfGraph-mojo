### Research Notes:

- Understand exactly how the shared representation contributes to the model
- Implementing custom operations or adjusting the graph in place.
- GAT vs GCN vs traditional layers
- Implementing an an attention SIMD vs Attention Layer
  - SIMD applies the samee operation across layers
  - Effectively a vectorization of the data ala pandas

### Research Goals:
- Batch the subgraphs




### Application Notes: